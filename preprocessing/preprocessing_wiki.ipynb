{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  second step replacement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z35M7FVPeUfK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset #import public datasets\n",
    "from tqdm import tqdm # for jupyter view\n",
    "import csv #for handle csv type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10 15:15:52,568 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import flair #NLP\n",
    "from flair.data import Sentence #typcial class in flair stands for sentence\n",
    "from flair.models import SequenceTagger #use for NER\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-large\") # load a pre-trian NER moduel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AMKGZrKwedL_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n#找不到anoanonymizer库\\nimport anonymizer\\nfrom anonymizer import entity\\nfrom anonymizer.core import initialize\\nfrom anonymizer.cache import NECache\\nimport anonymizer.entity.person as person\\nfrom anonymizer.entity.org import org, org_wiki\\nfrom anonymizer.entity.gpe import gpe, gpe_wiki\\nfrom functools import partial\\ninitialize()\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#找不到anoanonymizer库\n",
    "import anonymizer\n",
    "from anonymizer import entity\n",
    "from anonymizer.core import initialize\n",
    "from anonymizer.cache import NECache\n",
    "import anonymizer.entity.person as person\n",
    "from anonymizer.entity.org import org, org_wiki\n",
    "from anonymizer.entity.gpe import gpe, gpe_wiki\n",
    "from functools import partial\n",
    "initialize()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NU2d4uuPel5u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' #找不到anonymizer的库函数，所以重写一个\\ndef replace_entities_flair_wiki(text):\\n    # make example sentence\\n    sentence = Sentence(text)\\n    # predict NER tags\\n    tagger.predict(sentence)\\n    # iterate over entities and print\\n    replacements = [] #存储待替换实体信息\\n    replacement_map = {}\\n    #实体映射\\n    if not sentence.get_spans(\\'ner\\'):\\n        return text\\n\\n    for entity in sentence.get_spans(\\'ner\\'):\\n        if entity.text in replacement_map: ##已经在实体映射中有了\\n            replacements.append((entity.start_position, entity.end_position, replacement_map[entity.text], entity.text))\\n            ##存储替换信息\\n            continue\\n        if entity.get_label().value == \"ORG\":\\n            repl = org.handle(entity.text.split(\" \"), NECache()) ##从Wiki里面获取一个组织\\n            if not repl or \" \".join(repl) == entity.text:##为空或者和原来文本一样\\n                repl = org.handle(entity.text.split(\" \"), NECache())##重新选取一个\\n            if not repl or \" \".join(repl) == entity.text:\\n                continue  ##再次替换失败，则放弃这个\\n            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\\n            ##存储替换信息\\n            replacement_map[entity.text] = \" \".join(repl)\\n            ##存储实体映射\\n        elif entity.get_label().value == \"PER\":\\n            repl = person.handle(entity.text.split(\" \"), NECache())\\n            if not repl or \" \".join(repl) == entity.text:\\n                repl = person.handle(entity.text.split(\" \"), NECache())\\n            if not repl or \" \".join(repl) == entity.text:\\n                continue\\n            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\\n            replacement_map[entity.text] = \" \".join(repl)\\n        elif entity.get_label().value == \"LOC\":\\n            repl = gpe.handle([entity.text], {})\\n            if not repl or \" \".join(repl) == entity.text:\\n                repl = gpe.handle([entity.text], {})\\n            if not repl or \" \".join(repl) == entity.text:\\n                continue\\n            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\\n            replacement_map[entity.text] = \" \".join(repl)\\n\\n    #完成替换，将识别替换后的文本重新组合成一个完整的字符串\\n    if replacements:\\n        res = []\\n        i = 0\\n        s = text\\n        for (start, end, txt, orig) in replacements:\\n            assert orig != txt\\n            res.append(s[i:start] + txt)\\n            i = end\\n        res.append(s[end:])\\n        return \\'\\'.join(res)\\n    return text\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' #找不到anonymizer的库函数，所以重写一个\n",
    "def replace_entities_flair_wiki(text):\n",
    "    # make example sentence\n",
    "    sentence = Sentence(text)\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "    # iterate over entities and print\n",
    "    replacements = [] #存储待替换实体信息\n",
    "    replacement_map = {}\n",
    "    #实体映射\n",
    "    if not sentence.get_spans('ner'):\n",
    "        return text\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.text in replacement_map: ##已经在实体映射中有了\n",
    "            replacements.append((entity.start_position, entity.end_position, replacement_map[entity.text], entity.text))\n",
    "            ##存储替换信息\n",
    "            continue\n",
    "        if entity.get_label().value == \"ORG\":\n",
    "            repl = org.handle(entity.text.split(\" \"), NECache()) ##从Wiki里面获取一个组织\n",
    "            if not repl or \" \".join(repl) == entity.text:##为空或者和原来文本一样\n",
    "                repl = org.handle(entity.text.split(\" \"), NECache())##重新选取一个\n",
    "            if not repl or \" \".join(repl) == entity.text:\n",
    "                continue  ##再次替换失败，则放弃这个\n",
    "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
    "            ##存储替换信息\n",
    "            replacement_map[entity.text] = \" \".join(repl)\n",
    "            ##存储实体映射\n",
    "        elif entity.get_label().value == \"PER\":\n",
    "            repl = person.handle(entity.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == entity.text:\n",
    "                repl = person.handle(entity.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == entity.text:\n",
    "                continue\n",
    "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
    "            replacement_map[entity.text] = \" \".join(repl)\n",
    "        elif entity.get_label().value == \"LOC\":\n",
    "            repl = gpe.handle([entity.text], {})\n",
    "            if not repl or \" \".join(repl) == entity.text:\n",
    "                repl = gpe.handle([entity.text], {})\n",
    "            if not repl or \" \".join(repl) == entity.text:\n",
    "                continue\n",
    "            replacements.append((entity.start_position, entity.end_position, \" \".join(repl), entity.text))\n",
    "            replacement_map[entity.text] = \" \".join(repl)\n",
    "\n",
    "    #完成替换，将识别替换后的文本重新组合成一个完整的字符串\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        s = text\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(s[i:start] + txt)\n",
    "            i = end\n",
    "        res.append(s[end:])\n",
    "        return ''.join(res)\n",
    "    return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x1U9pJ3terdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "cls_data = load_dataset(\"imdb\")\n",
    "train_data = cls_data['train']\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import random\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# 定义一个全局变量来存储已查询过的实体及其类别信息\n",
    "cached_entities = {}\n",
    "\n",
    "def query_wikidata(entity_text, entity_type):\n",
    "    \"\"\"\n",
    "    通过SPARQL查询Wikidata,获取指定实体的标准标签\n",
    "    如果存在多个标签,随机选择一个作为替换文本\n",
    "    entity_text: 实体文本\n",
    "    entity_type: 实体类型,如\"ORG\"、\"PER\"等\n",
    "    \"\"\"\n",
    "    # 如果实体信息已经被缓存过，直接返回缓存的结果\n",
    "    if entity_text in cached_entities:\n",
    "        return cached_entities[entity_text]\n",
    "\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")  # 初始化SPARQL查询对象\n",
    "    query_template = \"\"\"\n",
    "SELECT ?item ?itemLabel\n",
    "WHERE {{\n",
    "    ?item wdt:P31 wd:%s. # 匹配实体类型\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }} # 使用英文标签\n",
    "}}\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "    query = query_template % entity_type_to_qid(entity_type)  # 格式化查询语句\n",
    "    sparql.setQuery(query)  # 设置查询语句\n",
    "    sparql.setReturnFormat(JSON)  # 设置返回JSON格式\n",
    "    results = sparql.query().convert() # 查询并解析结果\n",
    "\n",
    "    if results[\"results\"][\"bindings\"]:  # 如果查询结果不为空\n",
    "        # 构建一个包含所有候选标签的列表\n",
    "        candidate_labels = [binding[\"itemLabel\"][\"value\"] for binding in results[\"results\"][\"bindings\"]]\n",
    "        if len(candidate_labels) > 1:  # 如果存在多个候选标签\n",
    "            # 随机选择一个作为替换文本\n",
    "            repl = random.choice(candidate_labels)\n",
    "        else:\n",
    "            repl = candidate_labels[0]  # 只有一个候选标签,直接返回\n",
    "\n",
    "        # 将查询结果存入缓存\n",
    "        cached_entities[entity_text] = repl\n",
    "    else:\n",
    "        repl = entity_text  # 查询失败则返回原始实体文本\n",
    "\n",
    "    return repl\n",
    "\n",
    "def entity_type_to_qid(type):\n",
    "    \"\"\"\n",
    "    将实体类型映射到Wikidata的QID (Query ID)\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"ORG\": \"Q43229\",  # 组织机构\n",
    "        \"PER\": \"Q5\",      # 人物\n",
    "        \"LOC\": \"Q618123\"  # 地点\n",
    "    }\n",
    "    return mapping.get(type, None)  # 返回映射的QID，如果类型不存在则返回None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def replace_entities_flair_wiki(text):\n",
    "    sentence = Sentence(text)  # 创建一个Sentence对象\n",
    "    tagger.predict(sentence)  # 使用预定义的tagger预测命名实体\n",
    "    replacements = []  # 存储需要替换的实体信息\n",
    "    replacement_map = {}  # 存储已替换的实体映射\n",
    "\n",
    "    if not sentence.get_spans('ner'):  # 如果没有命名实体\n",
    "        return text  # 直接返回原始文本\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):  # 遍历每个命名实体\n",
    "        if entity.text in replacement_map:  # 如果实体已被替换过\n",
    "            # 将替换信息添加到replacements\n",
    "            replacements.append((entity.start_position, entity.end_position, replacement_map[entity.text], entity.text))\n",
    "            continue  # 继续处理下一个实体\n",
    "\n",
    "        # 根据实体类型查询Wikidata获取替换文本\n",
    "        if entity.get_label().value == \"ORG\":\n",
    "            repl = query_wikidata(entity.text, \"ORG\")\n",
    "        elif entity.get_label().value == \"PER\":\n",
    "            repl = query_wikidata(entity.text, \"PER\")\n",
    "        elif entity.get_label().value == \"LOC\":\n",
    "            repl = query_wikidata(entity.text, \"LOC\")\n",
    "        else:\n",
    "            continue  # 对其他类型实体不进行替换\n",
    "\n",
    "        # 如果查询结果不为空且不等于原实体文本\n",
    "        if repl and repl != entity.text:\n",
    "            # 将替换信息添加到replacements和replacement_map\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "            replacement_map[entity.text] = repl\n",
    "\n",
    "    if replacements:  # 如果有需要替换的实体\n",
    "        res = []  # 存储替换后的文本\n",
    "        i = 0  # 当前位置指针\n",
    "        s = text  # 原始文本\n",
    "        for start, end, txt, orig in replacements:  # 遍历每个替换信息\n",
    "            assert orig != txt  # 确保替换文本与原文本不同\n",
    "            res.append(s[i:start] + txt)  # 将位置指针前的原文本加上替换文本\n",
    "            i = end  # 更新位置指针\n",
    "        res.append(s[end:])  # 添加最后一段原文本\n",
    "        return ''.join(res)  # 将替换后的文本拼接并返回\n",
    "\n",
    "    return text  # 如果没有需要替换的实体,返回原始文本"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'SPARQL-QUERY: queryStr=\\n        SELECT ?item ?itemLabel\\n        WHERE {{\\n            ?item wdt:P31 wd:Q5. # \\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe5\\xae\\x9e\\xe4\\xbd\\x93\\xe7\\xb1\\xbb\\xe5\\x9e\\x8b\\n            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }} # \\xe4\\xbd\\xbf\\xe7\\x94\\xa8\\xe8\\x8b\\xb1\\xe6\\x96\\x87\\xe6\\xa0\\x87\\xe7\\xad\\xbe\\n        }}\\n        LIMIT 1000\\n        \\njava.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:119)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:889)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat org.wikidata.query.rdf.blazegraph.label.LabelServiceExtractOptimizer.lambda$optimizeJoinGroup$0(LabelServiceExtractOptimizer.java:44)\\n\\tat java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)\\n\\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\\n\\tat java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)\\n\\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\\n\\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\\n\\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\\n\\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\\n\\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\\n\\tat java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)\\n\\tat org.wikidata.query.rdf.blazegraph.label.LabelServiceExtractOptimizer.optimizeJoinGroup(LabelServiceExtractOptimizer.java:40)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.AbstractJoinGroupOptimizer.optimize(AbstractJoinGroupOptimizer.java:162)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.AbstractJoinGroupOptimizer.optimize(AbstractJoinGroupOptimizer.java:102)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.ASTOptimizerList.optimize(ASTOptimizerList.java:126)\\n\\tat com.bigdata.rdf.sparql.ast.eval.AST2BOpUtility.convert(AST2BOpUtility.java:269)\\n\\tat com.bigdata.rdf.sparql.ast.eval.ASTEvalHelper.optimizeQuery(ASTEvalHelper.java:426)\\n\\tat com.bigdata.rdf.sparql.ast.eval.ASTEvalHelper.evaluateTupleQuery(ASTEvalHelper.java:212)\\n\\tat com.bigdata.rdf.sail.BigdataSailTupleQuery.evaluate(BigdataSailTupleQuery.java:79)\\n\\tat com.bigdata.rdf.sail.BigdataSailTupleQuery.evaluate(BigdataSailTupleQuery.java:61)\\n\\tat org.openrdf.repository.sail.SailTupleQuery.evaluate(SailTupleQuery.java:75)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$TupleQueryTask.doQuery(BigdataRDFContext.java:1722)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.innerCall(BigdataRDFContext.java:1579)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.call(BigdataRDFContext.java:1544)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.call(BigdataRDFContext.java:757)\\n\\t... 4 more\\n'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001B[0m, in \u001B[0;36mSPARQLWrapper._query\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 926\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43murlopener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturnFormat\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:222\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    221\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:531\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    530\u001B[0m     meth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[1;32m--> 531\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:640\u001B[0m, in \u001B[0;36mHTTPErrorProcessor.http_response\u001B[1;34m(self, request, response)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m code \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m):\n\u001B[1;32m--> 640\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttp\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhdrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:569\u001B[0m, in \u001B[0;36mOpenerDirector.error\u001B[1;34m(self, proto, *args)\u001B[0m\n\u001B[0;32m    568\u001B[0m args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_error_default\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m orig_args\n\u001B[1;32m--> 569\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:502\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    501\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 502\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\urllib\\request.py:649\u001B[0m, in \u001B[0;36mHTTPDefaultErrorHandler.http_error_default\u001B[1;34m(self, req, fp, code, msg, hdrs)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[1;32m--> 649\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req\u001B[38;5;241m.\u001B[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001B[1;31mHTTPError\u001B[0m: HTTP Error 500: Internal Server Error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mEndPointInternalError\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m text\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIt was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mt go on to star in more and better films. Sadly, I didn\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mt think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCat\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124ms Meow\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and all his early ones from \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTargets\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m to \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNickleodeon\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124ms ex-girlfriend, Cybil Shepherd had a hit television series called \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMoonlighting\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mt no \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPaper Moon\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and only a very pale version of \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124ms Up, Doc\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m src\u001B[38;5;241m=\u001B[39m\u001B[43mreplace_entities_flair_wiki\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<br /><br />\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<br />\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m src\n",
      "Cell \u001B[1;32mIn[20], line 20\u001B[0m, in \u001B[0;36mreplace_entities_flair_wiki\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     18\u001B[0m     repl \u001B[38;5;241m=\u001B[39m query_wikidata(entity\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mORG\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m entity\u001B[38;5;241m.\u001B[39mget_label()\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPER\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 20\u001B[0m     repl \u001B[38;5;241m=\u001B[39m \u001B[43mquery_wikidata\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentity\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPER\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m entity\u001B[38;5;241m.\u001B[39mget_label()\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLOC\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     22\u001B[0m     repl \u001B[38;5;241m=\u001B[39m query_wikidata(entity\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLOC\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[19], line 24\u001B[0m, in \u001B[0;36mquery_wikidata\u001B[1;34m(entity_text, entity_type)\u001B[0m\n\u001B[0;32m     22\u001B[0m sparql\u001B[38;5;241m.\u001B[39msetQuery(query)  \u001B[38;5;66;03m# 设置查询语句\u001B[39;00m\n\u001B[0;32m     23\u001B[0m sparql\u001B[38;5;241m.\u001B[39msetReturnFormat(JSON)  \u001B[38;5;66;03m# 设置返回JSON格式\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43msparql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert() \u001B[38;5;66;03m# 查询并解析结果\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbindings\u001B[39m\u001B[38;5;124m\"\u001B[39m]:  \u001B[38;5;66;03m# 如果查询结果不为空\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# 构建一个包含所有候选标签的列表\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     candidate_labels \u001B[38;5;241m=\u001B[39m [binding[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitemLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m binding \u001B[38;5;129;01min\u001B[39;00m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbindings\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001B[0m, in \u001B[0;36mSPARQLWrapper.query\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    942\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquery\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQueryResult\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    943\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    944\u001B[0m \u001B[38;5;124;03m    Execute the query.\u001B[39;00m\n\u001B[0;32m    945\u001B[0m \u001B[38;5;124;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    958\u001B[0m \u001B[38;5;124;03m    :rtype: :class:`QueryResult` instance\u001B[39;00m\n\u001B[0;32m    959\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 960\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m QueryResult(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:938\u001B[0m, in \u001B[0;36mSPARQLWrapper._query\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    936\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m URITooLong(e\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m    937\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mcode \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m500\u001B[39m:\n\u001B[1;32m--> 938\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m EndPointInternalError(e\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m    939\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    940\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[1;31mEndPointInternalError\u001B[0m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'SPARQL-QUERY: queryStr=\\n        SELECT ?item ?itemLabel\\n        WHERE {{\\n            ?item wdt:P31 wd:Q5. # \\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe5\\xae\\x9e\\xe4\\xbd\\x93\\xe7\\xb1\\xbb\\xe5\\x9e\\x8b\\n            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }} # \\xe4\\xbd\\xbf\\xe7\\x94\\xa8\\xe8\\x8b\\xb1\\xe6\\x96\\x87\\xe6\\xa0\\x87\\xe7\\xad\\xbe\\n        }}\\n        LIMIT 1000\\n        \\njava.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:119)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:889)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: java.lang.ClassCastException: com.bigdata.rdf.sparql.ast.JoinGroupNode cannot be cast to com.bigdata.rdf.sparql.ast.StatementPatternNode\\n\\tat org.wikidata.query.rdf.blazegraph.label.LabelServiceExtractOptimizer.lambda$optimizeJoinGroup$0(LabelServiceExtractOptimizer.java:44)\\n\\tat java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)\\n\\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\\n\\tat java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)\\n\\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\\n\\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\\n\\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\\n\\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\\n\\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\\n\\tat java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)\\n\\tat org.wikidata.query.rdf.blazegraph.label.LabelServiceExtractOptimizer.optimizeJoinGroup(LabelServiceExtractOptimizer.java:40)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.AbstractJoinGroupOptimizer.optimize(AbstractJoinGroupOptimizer.java:162)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.AbstractJoinGroupOptimizer.optimize(AbstractJoinGroupOptimizer.java:102)\\n\\tat com.bigdata.rdf.sparql.ast.optimizers.ASTOptimizerList.optimize(ASTOptimizerList.java:126)\\n\\tat com.bigdata.rdf.sparql.ast.eval.AST2BOpUtility.convert(AST2BOpUtility.java:269)\\n\\tat com.bigdata.rdf.sparql.ast.eval.ASTEvalHelper.optimizeQuery(ASTEvalHelper.java:426)\\n\\tat com.bigdata.rdf.sparql.ast.eval.ASTEvalHelper.evaluateTupleQuery(ASTEvalHelper.java:212)\\n\\tat com.bigdata.rdf.sail.BigdataSailTupleQuery.evaluate(BigdataSailTupleQuery.java:79)\\n\\tat com.bigdata.rdf.sail.BigdataSailTupleQuery.evaluate(BigdataSailTupleQuery.java:61)\\n\\tat org.openrdf.repository.sail.SailTupleQuery.evaluate(SailTupleQuery.java:75)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$TupleQueryTask.doQuery(BigdataRDFContext.java:1722)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.innerCall(BigdataRDFContext.java:1579)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.call(BigdataRDFContext.java:1544)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataRDFContext$AbstractQueryTask.call(BigdataRDFContext.java:757)\\n\\t... 4 more\\n'"
     ]
    }
   ],
   "source": [
    "text='It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".'\n",
    "src=replace_entities_flair_wiki(text.replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "src"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5xaQ4wMne9KN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/25000 [04:28<266:37:22, 38.40s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/flair/imdb_train_flair_wiki.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        count = 0  # 初始化计数器\n",
    "        src = replace_entities_flair_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        writer.writerow((src, p['label']))\n",
    "        count+=1\n",
    "        if count>5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCJOegd3fkJ5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cls_data = load_dataset(\"cnn_dailymail\")\n",
    "train_data = cls_data['train']\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRmRR7Lkflr1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/flair/cnn_dm_train_flair_wiki.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"article\",\"highlights\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_flair_wiki(p['article'])\n",
    "        trg = replace_entities_flair_wiki(p['highlights'])\n",
    "        writer.writerow((src, trg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a22b859",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Spacy wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0ba6faff",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d826bb0e",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#找不到anonymizer\n",
    "def replace_entities_spacy_wiki(text):\n",
    "    parsed = nlp(text)\n",
    "    # iterate over entities and print\n",
    "    replacements = []\n",
    "    replacement_map = {}\n",
    "    if all([w.ent_type == 0 for w in parsed]):\n",
    "        return text\n",
    "\n",
    "    for word in parsed:\n",
    "        if word.text in replacement_map:\n",
    "            replacement_map[\"-\"] = \"-\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), replacement_map[word.text], word.text))\n",
    "            continue\n",
    "        if word.ent_type_ == \"ORG\":\n",
    "            repl = org.handle(word.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                repl = org.handle(word.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                continue\n",
    "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
    "            replacement_map[word.text] = \" \".join(repl)\n",
    "        elif word.ent_type_ == \"PERSON\":\n",
    "            repl = person.handle(word.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                repl = person.handle(word.text.split(\" \"), NECache())\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                continue\n",
    "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
    "            replacement_map[word.text] = \" \".join(repl)\n",
    "        elif word.ent_type_ == \"GPE\":\n",
    "            repl = gpe.handle([word.text], {})\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                repl = gpe.handle([word.text], {})\n",
    "            if not repl or \" \".join(repl) == word.text:\n",
    "                continue\n",
    "            replacements.append((word.idx, word.idx + len(word.text), \" \".join(repl), word.text))\n",
    "            replacement_map[word.text] = \" \".join(repl)\n",
    "            replacement_map[\"-\"] = \"-\"\n",
    "\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(text[i:start] + txt)\n",
    "#             print(\"\\\"\" + text[i:start] + \"\\\"\", \"\\\"\" + orig + \"\\\"\", \"\\\"\" + txt + \"\\\"\")\n",
    "            i = end\n",
    "        res.append(text[end:])\n",
    "        return ''.join(res)\n",
    "    return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_entities_spacy_wiki(text):\n",
    "    parsed = nlp(text)  # 使用预定义的nlp模型对文本进行处理\n",
    "    replacements = []  # 存储需要替换的实体信息\n",
    "    replacement_map = {}  # 存储已替换的实体映射\n",
    "\n",
    "    if all([w.ent_type == 0 for w in parsed]):  # 如果没有命名实体\n",
    "        return text  # 直接返回原始文本\n",
    "\n",
    "    for word in parsed:\n",
    "        if word.text in replacement_map:  # 如果实体已被替换过\n",
    "            replacement_map[\"-\"] = \"-\"  # 添加一个占位符\n",
    "            # 将替换信息添加到replacements\n",
    "            replacements.append((word.idx, word.idx + len(word.text), replacement_map[word.text], word.text))\n",
    "            continue  # 继续处理下一个实体\n",
    "\n",
    "        # 根据实体类型查询Wikidata获取替换文本\n",
    "        if word.ent_type_ == \"ORG\":\n",
    "            repl = query_wikidata(word.text, \"ORG\")\n",
    "        elif word.ent_type_ == \"PERSON\":\n",
    "            repl = query_wikidata(word.text, \"PERSON\")\n",
    "        elif word.ent_type_ == \"GPE\":\n",
    "            repl = query_wikidata(word.text, \"GPE\")\n",
    "        else:\n",
    "            continue  # 对其他类型实体不进行替换\n",
    "\n",
    "        # 如果查询结果不为空且不等于原实体文本\n",
    "        if repl and repl != word.text:\n",
    "            # 将替换信息添加到replacements和replacement_map\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "            replacement_map[word.text] = repl\n",
    "\n",
    "    replacement_map[\"-\"] = \"-\"  # 添加一个占位符\n",
    "\n",
    "    if replacements:  # 如果有需要替换的实体\n",
    "        res = []  # 存储替换后的文本\n",
    "        i = 0  # 当前位置指针\n",
    "        for start, end, txt, orig in replacements:  # 遍历每个替换信息\n",
    "            assert orig != txt  # 确保替换文本与原文本不同\n",
    "            res.append(text[i:start] + txt)  # 将位置指针前的原文本加上替换文本\n",
    "            i = end  # 更新位置指针\n",
    "        res.append(text[end:])  # 添加最后一段原文本\n",
    "        return ''.join(res)  # 将替换后的文本拼接并返回\n",
    "\n",
    "    return text  # 如果没有需要替换的实体,返回原始文本"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GoSMPtJMf8H7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "cls_data = load_dataset(\"imdb\")\n",
    "train_data = cls_data['train']\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2e0f816f",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m writer\u001B[38;5;241m.\u001B[39mwriterow([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m tqdm(train_data):\n\u001B[1;32m----> 5\u001B[0m     src \u001B[38;5;241m=\u001B[39m \u001B[43mreplace_entities_spacy_wiki\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<br /><br />\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<br />\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwriterow((src, p[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n",
      "Cell \u001B[1;32mIn[14], line 31\u001B[0m, in \u001B[0;36mreplace_entities_spacy_wiki\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     29\u001B[0m     replacement_map[word\u001B[38;5;241m.\u001B[39mtext] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(repl)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m word\u001B[38;5;241m.\u001B[39ment_type_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPE\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 31\u001B[0m     repl \u001B[38;5;241m=\u001B[39m \u001B[43mgpe\u001B[49m\u001B[38;5;241m.\u001B[39mhandle([word\u001B[38;5;241m.\u001B[39mtext], {})\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m repl \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(repl) \u001B[38;5;241m==\u001B[39m word\u001B[38;5;241m.\u001B[39mtext:\n\u001B[0;32m     33\u001B[0m         repl \u001B[38;5;241m=\u001B[39m gpe\u001B[38;5;241m.\u001B[39mhandle([word\u001B[38;5;241m.\u001B[39mtext], {})\n",
      "\u001B[1;31mNameError\u001B[0m: name 'gpe' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"data/spacy/imdb_train_spacy_wiki.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_spacy_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Tb8PVUBf5Co",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/spacy/imdb_train_spacy_wiki.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_spacy_wiki(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhWb5YNjf5Cp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cls_data = load_dataset(\"cnn_dailymail\")\n",
    "train_data = cls_data['train']\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bktO118kf5Cp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/spacy/cnn_dm_train_spacy_wiki.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"article\",\"highlights\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_spacy_wiki(p['article'])\n",
    "        trg = replace_entities_spacy_wiki(p['highlights'])\n",
    "        writer.writerow((src, trg))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}