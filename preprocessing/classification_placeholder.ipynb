{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7160da45",
   "metadata": {
    "id": "7160da45",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6f11e",
   "metadata": {
    "id": "81a6f11e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import flair #\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-large\") #加载了预训练NER模型\n",
    "#model_name_or_path=r\"D:\\My Computer\\py project\\last\\model\\flair\\pytorch_model.bin\"\n",
    "#tagger= SequenceTagger.load(model_name_or_path) ##本地模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adacd3",
   "metadata": {
    "id": "77adacd3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f571c8d",
   "metadata": {
    "id": "0f571c8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_entities_placeholder_flair(text):\n",
    "    # 用占位符来替换NER\n",
    "    # make example sentence\n",
    "    sentence = Sentence(text)\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "    # iterate over entities and print\n",
    "    replacements = [] #存储待替换的实体信息\n",
    "    if not sentence.get_spans('ner'):\n",
    "        return text\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        #对于识别到的命名实体\n",
    "        if entity.get_label().value == \"ORG\":\n",
    "            repl = \"ORG\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "            #将替换信息（起始位置、结束位置、替换后的值、原始文本）存储到replacements列表中\n",
    "        elif entity.get_label().value == \"PER\":\n",
    "            repl = \"PERSON\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "        elif entity.get_label().value == \"LOC\":\n",
    "            repl = \"LOCATION\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "\n",
    "    #完成替换，将识别替换后的文本重新组合成一个完整的字符串\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(text[i:start] + txt)\n",
    "            i = end\n",
    "        res.append(text[end:])\n",
    "        return ''.join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec94168",
   "metadata": {
    "tags": [],
    "id": "0ec94168",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't find cache for imdb for config 'default'\nAvailable configs in the cache: ['plain_text']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m----> 2\u001B[0m cls_data \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimdb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\datasets\\load.py:2548\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2543\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2544\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2545\u001B[0m )\n\u001B[0;32m   2547\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2548\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2550\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2551\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2557\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2558\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2559\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2560\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2561\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_default_config_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   2562\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2563\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2565\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2566\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\datasets\\load.py:2257\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   2255\u001B[0m builder_cls \u001B[38;5;241m=\u001B[39m get_dataset_builder_class(dataset_module, dataset_name\u001B[38;5;241m=\u001B[39mdataset_name)\n\u001B[0;32m   2256\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[1;32m-> 2257\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2263\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2264\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2265\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2266\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2268\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbuilder_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2269\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2270\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2271\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001B[0;32m   2273\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\datasets\\packaged_modules\\cache\\cache.py:122\u001B[0m, in \u001B[0;36mCache.__init__\u001B[1;34m(self, cache_dir, dataset_name, config_name, version, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhash\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m version \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;66;03m# First we try to find a folder that takes the config_kwargs into account\u001B[39;00m\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;66;03m# e.g. with \"default-data_dir=data%2Ffortran\" as config_id\u001B[39;00m\n\u001B[0;32m    119\u001B[0m     config_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBUILDER_CONFIG_CLASS(config_name \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcreate_config_id(\n\u001B[0;32m    120\u001B[0m         config_kwargs\u001B[38;5;241m=\u001B[39mconfig_kwargs, custom_features\u001B[38;5;241m=\u001B[39mfeatures\n\u001B[0;32m    121\u001B[0m     )\n\u001B[1;32m--> 122\u001B[0m     config_name, version, \u001B[38;5;28mhash\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43m_find_hash_in_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhash\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m version \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPass both hash=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and version=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\privacy-preserving-nlp\\lib\\site-packages\\datasets\\packaged_modules\\cache\\cache.py:48\u001B[0m, in \u001B[0;36m_find_hash_in_cache\u001B[1;34m(dataset_name, config_name, cache_dir)\u001B[0m\n\u001B[0;32m     38\u001B[0m         cached_directory_paths \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     39\u001B[0m             cached_directory_path\n\u001B[0;32m     40\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m cached_directory_path \u001B[38;5;129;01min\u001B[39;00m glob\u001B[38;5;241m.\u001B[39mglob(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(cached_directory_path)\n\u001B[0;32m     44\u001B[0m         ]\n\u001B[0;32m     45\u001B[0m     available_configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\n\u001B[0;32m     46\u001B[0m         {Path(cached_directory_path)\u001B[38;5;241m.\u001B[39mparts[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m cached_directory_path \u001B[38;5;129;01min\u001B[39;00m cached_directory_paths}\n\u001B[0;32m     47\u001B[0m     )\n\u001B[1;32m---> 48\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     49\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find cache for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     50\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for config \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config_name \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     51\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAvailable configs in the cache: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavailable_configs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m available_configs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     52\u001B[0m     )\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# get most recent\u001B[39;00m\n\u001B[0;32m     54\u001B[0m cached_directory_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28msorted\u001B[39m(cached_directory_paths, key\u001B[38;5;241m=\u001B[39m_get_modification_time)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[1;31mValueError\u001B[0m: Couldn't find cache for imdb for config 'default'\nAvailable configs in the cache: ['plain_text']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cls_data = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ee403d",
   "metadata": {
    "tags": [],
    "id": "25ee403d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mcls_data\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cls_data' is not defined"
     ]
    }
   ],
   "source": [
    "cls_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab197dd",
   "metadata": {
    "tags": [],
    "id": "2ab197dd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mcls_data\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      2\u001B[0m unsup_data \u001B[38;5;241m=\u001B[39m cls_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munsupervised\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      3\u001B[0m test_data \u001B[38;5;241m=\u001B[39m cls_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cls_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = cls_data['train']\n",
    "unsup_data = cls_data['unsupervised']\n",
    "test_data = cls_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be03cfa6",
   "metadata": {
    "tags": [],
    "id": "be03cfa6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_data\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244e6606",
   "metadata": {
    "id": "244e6606",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'It was great to see some of my favorite stars of 30 years ago including PERSON, PERSON and PERSON. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing. Some of the smaller female roles were fine, PERSON and PERSON were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think PERSON got a chance to act in this her only important film role. The film appears to have some fans, and I was very open-minded when I started watching it. I am a big PERSON fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one. It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, PERSON\\'s ex-girlfriend, PERSON had a hit television series called \"Moonlighting\" stealing the story idea from PERSON. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines. Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_entities_placeholder_flair(train_data[10]['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "#预处理后传给替换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d224c49",
   "metadata": {
    "tags": [],
    "id": "3d224c49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [9:34:09<00:00,  1.38s/it]   \n"
     ]
    }
   ],
   "source": [
    "train_pairs_placeholder = []\n",
    "#匿名化ibdm的训练集\n",
    "with open(\"data/flair/imdb_train.csv\", \"w\",encoding='utf-8') as f:\n",
    "    #打开文件\n",
    "    writer = csv.writer(f)\n",
    "    #打开csv\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        #对信息进行预处理\n",
    "        train_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f1c557",
   "metadata": {
    "id": "e4f1c557",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [1:23:24<00:00,  5.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "test_pairs_placeholder = []\n",
    "#匿名化测试集\n",
    "with open(\"data/flair/imdb_test.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(test_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        test_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cebd1177",
   "metadata": {
    "id": "cebd1177",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [2:53:05<00:00,  4.81it/s]  \n"
     ]
    }
   ],
   "source": [
    "unsup_pairs_placeholder = []\n",
    "#匿名化未标记的数据集\n",
    "with open(\"data/flair/imdb_unsup.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(unsup_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        unsup_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea0559",
   "metadata": {
    "id": "01ea0559",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3cff6",
   "metadata": {
    "id": "34e3cff6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')#安装spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f7ead",
   "metadata": {
    "id": "6e2f7ead",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_entities_placeholder_spacy(text):\n",
    "    parsed = nlp(text)\n",
    "    # iterate over entities and print\n",
    "    replacements = []\n",
    "    if all([w.ent_type == 0 for w in parsed]):\n",
    "        return text\n",
    "\n",
    "    for word in parsed:\n",
    "        if word.ent_type_ == \"ORG\":\n",
    "            repl = \"ORG\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "        elif word.ent_type_ == \"PERSON\":\n",
    "            repl = \"PERSON\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "        elif word.ent_type_ == \"GPE\":\n",
    "            repl = \"LOCATION\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(text[i:start] + txt)\n",
    "            i = end\n",
    "        res.append(text[end:])\n",
    "        return ''.join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafabdc3",
   "metadata": {
    "id": "aafabdc3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c5de4",
   "metadata": {
    "id": "541c5de4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_train.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        train_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908c61f",
   "metadata": {
    "id": "9908c61f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_test.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(test_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        test_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7bfb2",
   "metadata": {
    "id": "13f7bfb2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unsup_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_unsup.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(unsup_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        unsup_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "privacy-preserving-nlp",
   "language": "python",
   "display_name": "privacy-preserving-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}