{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7160da45",
   "metadata": {
    "id": "7160da45",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a6f11e",
   "metadata": {
    "id": "81a6f11e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-04 22:45:47,274 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import flair #\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-large\") #加载了预训练NER模型\n",
    "#model_name_or_path=r\"D:\\My Computer\\py project\\last\\model\\flair\\pytorch_model.bin\"\n",
    "#tagger= SequenceTagger.load(model_name_or_path) ##本地模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77adacd3",
   "metadata": {
    "id": "77adacd3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f571c8d",
   "metadata": {
    "id": "0f571c8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_entities_placeholder_flair(text):\n",
    "    # 用占位符来替换NER\n",
    "    # make example sentence\n",
    "    sentence = Sentence(text)\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "    # iterate over entities and print\n",
    "    replacements = [] #存储待替换的实体信息\n",
    "    if not sentence.get_spans('ner'):\n",
    "        return text\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        #对于识别到的命名实体\n",
    "        if entity.get_label().value == \"ORG\":\n",
    "            repl = \"ORG\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "            #将替换信息（起始位置、结束位置、替换后的值、原始文本）存储到replacements列表中\n",
    "        elif entity.get_label().value == \"PER\":\n",
    "            repl = \"PERSON\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "        elif entity.get_label().value == \"LOC\":\n",
    "            repl = \"LOCATION\"\n",
    "            replacements.append((entity.start_position, entity.end_position, repl, entity.text))\n",
    "\n",
    "    #完成替换，将识别替换后的文本重新组合成一个完整的字符串\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(text[i:start] + txt)\n",
    "            i = end\n",
    "        res.append(text[end:])\n",
    "        return ''.join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec94168",
   "metadata": {
    "tags": [],
    "id": "0ec94168",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "cls_data = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ee403d",
   "metadata": {
    "tags": [],
    "id": "25ee403d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab197dd",
   "metadata": {
    "tags": [],
    "id": "2ab197dd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = cls_data['train']\n",
    "unsup_data = cls_data['unsupervised']\n",
    "test_data = cls_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be03cfa6",
   "metadata": {
    "tags": [],
    "id": "be03cfa6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?<br /><br />\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244e6606",
   "metadata": {
    "id": "244e6606",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'It was great to see some of my favorite stars of 30 years ago including PERSON, PERSON and PERSON. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing. Some of the smaller female roles were fine, PERSON and PERSON were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think PERSON got a chance to act in this her only important film role. The film appears to have some fans, and I was very open-minded when I started watching it. I am a big PERSON fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one. It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, PERSON\\'s ex-girlfriend, PERSON had a hit television series called \"Moonlighting\" stealing the story idea from PERSON. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines. Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_entities_placeholder_flair(train_data[10]['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "#预处理后传给替换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d224c49",
   "metadata": {
    "tags": [],
    "id": "3d224c49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [9:34:09<00:00,  1.38s/it]   \n"
     ]
    }
   ],
   "source": [
    "train_pairs_placeholder = []\n",
    "#匿名化ibdm的训练集\n",
    "with open(\"data/flair/imdb_train.csv\", \"w\",encoding='utf-8') as f:\n",
    "    #打开文件\n",
    "    writer = csv.writer(f)\n",
    "    #打开csv\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        #对信息进行预处理\n",
    "        train_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1c557",
   "metadata": {
    "id": "e4f1c557",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_pairs_placeholder = []\n",
    "#匿名化测试集\n",
    "with open(\"data/flair/imdb_test.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(test_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        test_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd1177",
   "metadata": {
    "id": "cebd1177",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unsup_pairs_placeholder = []\n",
    "#匿名化未标记的数据集\n",
    "with open(\"data/flair/imdb_unsup.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(unsup_data):\n",
    "        src = replace_entities_placeholder_flair(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        unsup_pairs_placeholder.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea0559",
   "metadata": {
    "id": "01ea0559",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3cff6",
   "metadata": {
    "id": "34e3cff6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')#安装spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f7ead",
   "metadata": {
    "id": "6e2f7ead",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_entities_placeholder_spacy(text):\n",
    "    parsed = nlp(text)\n",
    "    # iterate over entities and print\n",
    "    replacements = []\n",
    "    if all([w.ent_type == 0 for w in parsed]):\n",
    "        return text\n",
    "\n",
    "    for word in parsed:\n",
    "        if word.ent_type_ == \"ORG\":\n",
    "            repl = \"ORG\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "        elif word.ent_type_ == \"PERSON\":\n",
    "            repl = \"PERSON\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "        elif word.ent_type_ == \"GPE\":\n",
    "            repl = \"LOCATION\"\n",
    "            replacements.append((word.idx, word.idx + len(word.text), repl, word.text))\n",
    "\n",
    "    if replacements:\n",
    "        res = []\n",
    "        i = 0\n",
    "        for (start, end, txt, orig) in replacements:\n",
    "            assert orig != txt\n",
    "            res.append(text[i:start] + txt)\n",
    "            i = end\n",
    "        res.append(text[end:])\n",
    "        return ''.join(res)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafabdc3",
   "metadata": {
    "id": "aafabdc3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c5de4",
   "metadata": {
    "id": "541c5de4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_train.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(train_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        train_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908c61f",
   "metadata": {
    "id": "9908c61f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_test.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(test_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        test_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7bfb2",
   "metadata": {
    "id": "13f7bfb2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unsup_pairs_placeholder2 = []\n",
    "with open(\"data/spacy/imdb_unsup.csv\", \"w\",encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"text\",\"label\"])\n",
    "    for p in tqdm(unsup_data):\n",
    "        src = replace_entities_placeholder_spacy(p['text'].replace(\"<br /><br />\", \" \").replace(\"<br />\", \"\"))\n",
    "        unsup_pairs_placeholder2.append((src, p['label']))\n",
    "        writer.writerow((src, p['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "privacy-preserving-nlp",
   "language": "python",
   "display_name": "privacy-preserving-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}